# Cyberbullying Data Preparation

## Main idea
- Data is stored in GCP (cybulde-data-versioning repo)
- Process this data parallely to be in a format to be picked up by the DL model
- Train tokenizer on this dataset

## Python script
-

 
## Docker
The docker container should be able to:
- Fetch data from GCP
- Create VMs to process the data parallelly



 
